from torch import nn as nn
import torch
import rlkit.torch.pytorch_util as ptu
import gtimer as gt
from rlkit.core.rl_algorithm import BaseRLAlgorithm
from rlkit.data_management.replay_buffer import ReplayBuffer
from rlkit.samplers.data_collector import PathCollector
import abc
import tqdm


class BatchRLAlgorithm(BaseRLAlgorithm, metaclass=abc.ABCMeta):
    def __init__(
            self,
            trainer,
            exploration_env,
            evaluation_env,
            exploration_data_collector: PathCollector,
            evaluation_data_collector: PathCollector,
            replay_buffer: ReplayBuffer,
            batch_size,
            max_path_length,
            num_epochs,
            num_eval_steps_per_epoch,
            num_expl_steps_per_train_loop,
            num_trains_per_train_loop,
            num_train_loops_per_epoch=1,
            min_num_steps_before_training=0,
            start_epoch=0,  # negative epochs are offline, positive epochs are online
            name='default'
    ):
        super().__init__(
            trainer,
            exploration_env,
            evaluation_env,
            exploration_data_collector,
            evaluation_data_collector,
            replay_buffer,
            name
        )
        self.batch_size = batch_size
        self.max_path_length = max_path_length
        self.num_epochs = num_epochs
        self.num_eval_steps_per_epoch = num_eval_steps_per_epoch
        self.num_trains_per_train_loop = num_trains_per_train_loop
        self.num_train_loops_per_epoch = num_train_loops_per_epoch
        self.num_expl_steps_per_train_loop = num_expl_steps_per_train_loop
        self.min_num_steps_before_training = min_num_steps_before_training
        self._start_epoch = start_epoch
        self.name = name
    
    def fuse(self, other):
        self.trainer.fuse(other.trainer)
    
    def get_networks(self):
        return self.trainer.networks[1:]
    
    def get_stats(self):
        return self.trainer.get_stats()
    
    def set_networks(self, networks):
        self.trainer.set_networks(networks)
    
    def to(self, device):
        self.trainer.to(device)
       
    def step(self, epoch):
        """Negative epochs are offline, positive epochs are online"""
        # for self.epoch in gt.timed_for(
        #         range(self._start_epoch, self.num_epochs),
        #         save_itrs=True,
        # ):

        offline_rl = epoch < 0
        self.to(ptu.device)

        self._begin_epoch(epoch)
        self._step(epoch, offline_rl)
        self._end_epoch(epoch)
        #self.trainer.to('cpu')

    def _step(self, epoch, offline_rl):
        if epoch == 0 and self.min_num_steps_before_training > 0:
            init_expl_paths = self.expl_data_collector.collect_new_paths(
                self.max_path_length,
                self.min_num_steps_before_training,
                discard_incomplete_paths=False,
            )
            if not offline_rl:
                self.replay_buffer.add_paths(init_expl_paths)
            self.expl_data_collector.end_epoch(-1)

        self.eval_data_collector.collect_new_paths(
            self.max_path_length,
            self.num_eval_steps_per_epoch,
            discard_incomplete_paths=True,
        )
        gt.stamp(f'client_{self.client_id}_epoch_{epoch}_evaluation_sampling', unique=True)

        for _ in tqdm.tqdm(range(self.num_train_loops_per_epoch), desc='num_train_loops'):
            new_expl_paths = self.expl_data_collector.collect_new_paths(
                self.max_path_length,
                self.num_expl_steps_per_train_loop,
                discard_incomplete_paths=False,
            )
            gt.stamp(f'client_{self.client_id}_epoch_{epoch}_exploration_sampling', unique=True)

            if not offline_rl:
                self.replay_buffer.add_paths(new_expl_paths)
            gt.stamp(f'client_{self.client_id}_epoch_{epoch}_data_storing', unique=True)

            self.training_mode(True)
            gt.stamp(f'client_{self.client_id}_epoch_{epoch}_training_start', unique=True)  # æ–°å¢
            # è°ƒè¯•ä»£ç 
            print(f"\nğŸ” [å®¢æˆ·ç«¯ {self.client_id}] å¼€å§‹è®­ç»ƒå¾ªç¯è°ƒè¯•")
            print(f"Replay Buffer å¤§å°: {self.replay_buffer.num_steps_can_sample()}")

            # æµ‹è¯•é‡‡æ ·ä¸€ä¸ªå°æ‰¹æ¬¡
            try:
                print("ğŸ§ª æµ‹è¯•é‡‡æ ·å°æ‰¹æ¬¡...")
                test_batch = self.replay_buffer.random_batch(2)  # åªé‡‡æ ·2ä¸ªæ ·æœ¬
                print("âœ… å°æ‰¹æ¬¡é‡‡æ ·æˆåŠŸ:")
                for key, value in test_batch.items():
                    if isinstance(value, torch.Tensor):
                        print(f"  {key}: Tensor, shape={value.shape}, dtype={value.dtype}, device={value.device}")
                    elif isinstance(value, np.ndarray):
                        print(f"  {key}: ndarray, shape={value.shape}, dtype={value.dtype}")
                    else:
                        print(f"  {key}: {type(value)}")
            except Exception as e:
                print(f"âŒ å°æ‰¹æ¬¡é‡‡æ ·å¤±è´¥: {str(e)}")
                import traceback
                print(f"é”™è¯¯è¯¦æƒ…:\n{traceback.format_exc()}")
                raise
            for itr in tqdm.tqdm(range(self.num_trains_per_train_loop), desc='trains per train loop'):
                train_data = self.replay_buffer.random_batch(self.batch_size)
                self.trainer.train(train_data)

            gt.stamp(f'client_{self.client_id}_epoch_{epoch}_training_end', unique=True)  # æ–°å¢
            self.training_mode(False)


class TorchBatchRLAlgorithm(BatchRLAlgorithm):
    def __init__(self, client_id, **kwargs):
        super().__init__(**kwargs)
        self.client_id = client_id  # å­˜å‚¨å®¢æˆ·ç«¯ID
        self.name = f'client_{self.client_id}'
    def _end_epoch(self, epoch):
        """è¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œé¿å…ç”Ÿæˆé»˜è®¤åç§°"""
        snapshot = super()._get_snapshot()
        self.logger.save_itr_params(epoch, snapshot)
        gt.stamp(f'client_{self.client_id}_epoch_{epoch}_saving', unique=True)
        self._log_stats(epoch)
    def _log_stats(self, epoch):
        """å®Œå…¨è¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œé¿å…è°ƒç”¨çˆ¶ç±»ä¸­çš„ gt.stamp"""
        # 1. è®°å½•æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¿ç•™å¿…è¦é€»è¾‘ï¼‰
        self.logger.record_dict({"epoch": epoch}, step=epoch)
        self.logger.record_dict(
            self.replay_buffer.get_diagnostics(),
            prefix='replay_buffer/'
        )
        self.logger.record_dict(self.trainer.get_diagnostics(), prefix='trainer/', step=epoch)
        # ...ï¼ˆå…¶ä»–æ—¥å¿—è®°å½•é€»è¾‘ï¼ŒæŒ‰éœ€å¤åˆ¶çˆ¶ç±»ä»£ç ï¼‰...

        # 2. è‡ªå®šä¹‰æ—¶é—´æˆ³åç§°ï¼ˆåŒ…å«å®¢æˆ·ç«¯ ID å’Œè½®æ¬¡ï¼‰
        gt.stamp(f'client_{self.client_id}_epoch_{epoch}_logging', unique=True)

        # 3. ç¦ç”¨çˆ¶ç±»ä¸­çš„é»˜è®¤æ—¶é—´æˆ³è°ƒç”¨
        # ï¼ˆä¸å†è°ƒç”¨ super()._log_stats(epoch)ï¼‰

        # 4. è¾“å‡ºæ—¥å¿—
        self.logger.dump_tabular(with_prefix=False, with_timestamp=False)
    def to(self, device):
        for net in self.trainer.networks:
            net.to(device)

    def training_mode(self, mode):
        for net in self.trainer.networks:
            net.train(mode)