# main.py (ä¿®å¤rolloutæ¥å£é—®é¢˜ + é›†æˆAUCåˆ†æ)
import os
import logging
import torch
import click
import pandas as pd
import numpy as np
import rlkit.torch.pytorch_util as ptu
from data_loader import IntrusionDataLoader
from policy import ClassificationPolicy
from sac_trainer import SACClassifierTrainer
from fed_algorithm import FedAlgorithm
from intrusion_detection_env import IntrusionDetectionEnv 
from networks import ConcatMlp
from sac_algorithm import TorchBatchRLAlgorithm
from simple_replay_buffer import SimpleReplayBuffer
from fed_path_collector import FedPathCollector

# ========== æ–°å¢ï¼šå¯¼å…¥AUCåˆ†ææ¨¡å— ==========
from auc_analysis import analyze_auc_results, plot_auc_trends, generate_auc_report

# ä½¿ç”¨æ­£ç¡®çš„æ ‡ç­¾åˆ—
TARGET_COLUMN = "Label"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def initialize_q_networks(obs_dim, action_dim=2, hidden_sizes=[256,256], device='cpu'):
    input_dim = obs_dim + action_dim
    qf1 = ConcatMlp(input_dim, 1, hidden_sizes).to(device)
    qf2 = ConcatMlp(input_dim, 1, hidden_sizes).to(device)
    target_qf1 = ConcatMlp(input_dim, 1, hidden_sizes).to(device)
    target_qf2 = ConcatMlp(input_dim, 1, hidden_sizes).to(device)
    target_qf1.load_state_dict(qf1.state_dict())
    target_qf2.load_state_dict(qf2.state_dict())
    return qf1, qf2, target_qf1, target_qf2

def experiment(variant):
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        ptu.set_gpu_mode(True)
        logging.info(f"ä½¿ç”¨GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    else:
        ptu.set_gpu_mode(False)
        logging.info("ä½¿ç”¨CPUè®¾å¤‡")
    
    device = ptu.device
    algorithms = []
    
    # ================== åŠ¨æ€è·å–obs_dim ==================
    try:
        logging.info("å¼€å§‹ç‰¹å¾ç»´åº¦éªŒè¯...")
        dummy_loader = IntrusionDataLoader(
            client_id=0,
            data_root=variant['data_root'],
            batch_size=variant['batch_size']
        )
        obs_dim = dummy_loader.feature_dim
        logging.info(f"æˆåŠŸè·å–ç‰¹å¾ç»´åº¦: {obs_dim}")
        
        # é‡‡æ ·æµ‹è¯•æ•°æ®å¹¶æ£€æŸ¥ç»´åº¦
        features, labels = dummy_loader.sample_batch()
        logging.info(f"é‡‡æ ·ç‰¹å¾å½¢çŠ¶: {features.shape}, æ•°æ®ç±»å‹: {features.dtype}")
        logging.info(f"é‡‡æ ·æ ‡ç­¾å½¢çŠ¶: {labels.shape}, æ•°æ®ç±»å‹: {labels.dtype}")
        
        # éªŒè¯ç‰¹å¾ç»´åº¦æ˜¯å¦åŒ¹é…
        if features.shape[1] != obs_dim:
            raise ValueError(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…! é¢„æœŸ: {obs_dim}, å®é™…: {features.shape[1]}")
            
    except Exception as e:
        logging.error(f"æ— æ³•è·å–ç‰¹å¾ç»´åº¦: {str(e)}")
        raise
    
    # ================== å¢å¼ºçš„æ ‡ç­¾åˆ†å¸ƒæ£€æŸ¥ ==================
    try:
        logging.info("å¼€å§‹éªŒè¯å®¢æˆ·ç«¯æ•°æ®æ ‡ç­¾åˆ†å¸ƒ...")
        for client_id in range(variant['num_agents']):
            train_path = os.path.join(variant['data_root'], f"client_{client_id}/train.csv")
            if not os.path.exists(train_path):
                logging.warning(f"å®¢æˆ·ç«¯{client_id} è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨")
                continue
                
            client_train = pd.read_csv(train_path)
            
            # æ£€æŸ¥æ ‡ç­¾åˆ—æ˜¯å¦å­˜åœ¨
            if TARGET_COLUMN not in client_train.columns:
                raise ValueError(f"å®¢æˆ·ç«¯{client_id} æ•°æ®ç¼ºå°‘æ ‡ç­¾åˆ— {TARGET_COLUMN}")
            
            # æ£€æŸ¥æ ‡ç­¾å€¼æ˜¯å¦åˆæ³•
            label_values = client_train[TARGET_COLUMN].unique()
            invalid_values = [v for v in label_values if v not in (0, 1)]
            if invalid_values:
                raise ValueError(f"å®¢æˆ·ç«¯{client_id} åŒ…å«éæ³•æ ‡ç­¾å€¼: {invalid_values}")
            
            # æ£€æŸ¥ç±»åˆ«å¹³è¡¡æ€§
            label_counts = client_train[TARGET_COLUMN].value_counts()
            imbalance_ratio = label_counts.max() / label_counts.min()
            
            logging.info(f"å®¢æˆ·ç«¯{client_id} æ ‡ç­¾åˆ†å¸ƒ:\n{label_counts.to_string()}")
            if imbalance_ratio > 10:
                logging.warning(f"å®¢æˆ·ç«¯{client_id} æ ‡ç­¾ä¸¥é‡ä¸å¹³è¡¡ (æ¯”ä¾‹: {imbalance_ratio:.1f}:1)")
    except Exception as e:
        logging.error(f"æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
        raise
    
    # ================== æ•°æ®éªŒè¯æ­¥éª¤ ==================
    logging.info("å¼€å§‹æ•°æ®éªŒè¯...")
    for client_id in range(variant['num_agents']):
        try:
            data_loader = IntrusionDataLoader(
                client_id=client_id,
                data_root=variant['data_root'],
                batch_size=variant['batch_size']
            )
            
            # é‡‡æ ·æµ‹è¯•æ‰¹æ¬¡
            features, labels = data_loader.sample_batch()
            
            # éªŒè¯ç‰¹å¾
            if not isinstance(features, np.ndarray):
                raise TypeError(f"ç‰¹å¾åº”ä¸ºNumPyæ•°ç»„, å®é™…ç±»å‹: {type(features)}")
            if features.dtype != np.float32:
                raise TypeError(f"ç‰¹å¾åº”ä¸ºfloat32, å®é™…ç±»å‹: {features.dtype}")
            
            # éªŒè¯æ ‡ç­¾
            if not isinstance(labels, np.ndarray):
                raise TypeError(f"æ ‡ç­¾åº”ä¸ºNumPyæ•°ç»„, å®é™…ç±»å‹: {type(labels)}")
            if labels.dtype != np.int64:
                raise TypeError(f"æ ‡ç­¾åº”ä¸ºint64, å®é™…ç±»å‹: {labels.dtype}")
                
            logging.info(f"å®¢æˆ·ç«¯{client_id} æ•°æ®éªŒè¯é€šè¿‡")
        except Exception as e:
            logging.error(f"å®¢æˆ·ç«¯{client_id} æ•°æ®éªŒè¯å¤±è´¥: {str(e)}")
            # è®°å½•é—®é¢˜æ•°æ®
            try:
                problem_path = os.path.join(variant['data_root'], f"client_{client_id}/train.csv")
                logging.error(f"é—®é¢˜æ•°æ®è·¯å¾„: {problem_path}")
                if os.path.exists(problem_path):
                    problem_df = pd.read_csv(problem_path)
                    logging.error(f"æ•°æ®åˆ—ç±»å‹:\n{problem_df.dtypes}")
                    logging.error(f"å‰5è¡Œæ•°æ®:\n{problem_df.head().to_string()}")
            except Exception as e:
                logging.error(f"æ— æ³•è®°å½•é—®é¢˜æ•°æ®è¯¦æƒ…: {str(e)}")
            raise
    
    # ================== ç»´åº¦ç«¯åˆ°ç«¯éªŒè¯ ==================
    logging.info("="*50)
    logging.info("å¼€å§‹ç«¯åˆ°ç«¯ç»´åº¦éªŒè¯")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        test_loader = IntrusionDataLoader(
            client_id=0,
            data_root=variant['data_root'],
            batch_size=variant['batch_size']
        )
        
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
        test_env = IntrusionDetectionEnv(
            data_loader=test_loader,
            max_steps=variant['max_steps']
        )
        
        # æµ‹è¯•ç¯å¢ƒé‡ç½®
        obs = test_env.reset()
        logging.info(f"ç¯å¢ƒé‡ç½®è¿”å›è§‚å¯Ÿå€¼å½¢çŠ¶: {obs.shape}")
        
        # éªŒè¯è§‚å¯Ÿå€¼ç»´åº¦
        if obs.shape[0] != obs_dim:
            raise ValueError(f"ç¯å¢ƒè¿”å›è§‚å¯Ÿå€¼ç»´åº¦é”™è¯¯: é¢„æœŸ {obs_dim}, å®é™… {obs.shape[0]}")
        
        # æµ‹è¯•ç¯å¢ƒæ­¥éª¤
        action = 0
        next_obs, reward, done, _ = test_env.step(action)
        logging.info(f"ç¯å¢ƒæ­¥éª¤è¿”å›ä¸‹ä¸€è§‚å¯Ÿå€¼å½¢çŠ¶: {next_obs.shape}")
        
        # éªŒè¯ä¸‹ä¸€è§‚å¯Ÿå€¼ç»´åº¦
        if next_obs.shape[0] != obs_dim:
            raise ValueError(f"ç¯å¢ƒè¿”å›ä¸‹ä¸€è§‚å¯Ÿå€¼ç»´åº¦é”™è¯¯: é¢„æœŸ {obs_dim}, å®é™… {next_obs.shape[0]}")
        
        # åˆ›å»ºæµ‹è¯•ç­–ç•¥ç½‘ç»œ
        test_policy = ClassificationPolicy(
            obs_dim=obs_dim,
            hidden_sizes=variant['policy_hidden_sizes'],
            layer_normalization=variant['layer_norm']
        ).to(device)
        
        # æµ‹è¯•ç­–ç•¥ç½‘ç»œ
        action, log_prob = test_policy.get_action(obs)
        logging.info(f"ç­–ç•¥ç½‘ç»œè¿”å›åŠ¨ä½œ: {action}, æ—¥å¿—æ¦‚ç‡: {log_prob}")
        
        # ============= æ–°å¢ï¼šæµ‹è¯•ç¯å¢ƒspecå±æ€§ =============
        logging.info("æµ‹è¯•ç¯å¢ƒspecå±æ€§...")
        env_spec = test_env.spec
        logging.info(f"ç¯å¢ƒspec - è§‚å¯Ÿç©ºé—´: {env_spec.observation_space.shape}")
        logging.info(f"ç¯å¢ƒspec - åŠ¨ä½œç©ºé—´: {env_spec.action_space.n}")
        logging.info(f"ç¯å¢ƒspec - æœ€å¤§æ­¥æ•°: {env_spec.max_episode_steps}")
        
        # ============= æ–°å¢ï¼šæµ‹è¯•è·¯å¾„æ”¶é›†å™¨æ¥å£ =============
        logging.info("æµ‹è¯•è·¯å¾„æ”¶é›†å™¨æ¥å£...")
        test_collector = FedPathCollector(
            policy=test_policy,
            device=device
            # æ³¨æ„ï¼šä¸ä¼ å…¥rollout_fnï¼Œä½¿ç”¨é»˜è®¤çš„è‡ªå®šä¹‰rollout
        )
        test_collector.set_data_loader(test_loader)
        
        # æµ‹è¯•æ”¶é›†å°‘é‡è·¯å¾„
        test_paths = test_collector.collect_new_paths(
            max_path_length=10,
            num_steps=20
        )
        logging.info(f"æˆåŠŸæ”¶é›†æµ‹è¯•è·¯å¾„: {len(test_paths)} æ¡")
        
        logging.info("ç«¯åˆ°ç«¯ç»´åº¦éªŒè¯æˆåŠŸ")
    except Exception as e:
        logging.error(f"ç«¯åˆ°ç«¯ç»´åº¦éªŒè¯å¤±è´¥: {str(e)}")
        raise
    finally:
        logging.info("="*50)
    
    # ================== åˆå§‹åŒ–å„å®¢æˆ·ç«¯ç®—æ³• ==================
    for client_id in range(variant['num_agents']):
        try:
            logging.info(f"åˆå§‹åŒ–å®¢æˆ·ç«¯ {client_id} ç®—æ³•...")
            
            data_loader = IntrusionDataLoader(
                client_id=client_id,
                data_root=variant['data_root'],
                batch_size=variant['batch_size']
            )
            env = IntrusionDetectionEnv(
                data_loader=data_loader,
                max_steps=variant['max_steps']
            )
            
            policy = ClassificationPolicy(
                obs_dim=obs_dim,
                hidden_sizes=variant['policy_hidden_sizes'],
                layer_normalization=variant['layer_norm']
            ).to(device)
            
            qf1, qf2, target_qf1, target_qf2 = initialize_q_networks(obs_dim, device=device)
            
            # æ ¹æ®ä¸å¹³è¡¡åº¦è°ƒæ•´ç±»åˆ«æƒé‡
            label_counts = data_loader.train_df[TARGET_COLUMN].value_counts()
            minority_class = 1 if label_counts[0] > label_counts[1] else 0
            majority_weight = 1.0
            minority_weight = max(5.0, label_counts.max() / label_counts.min())
            
            logging.info(f"å®¢æˆ·ç«¯{client_id} ç±»åˆ«æƒé‡: 0={majority_weight:.1f}, 1={minority_weight:.1f}")
            
            trainer = SACClassifierTrainer(
                client_id,
                env,
                policy,
                qf1,
                qf2,
                target_qf1,
                target_qf2,
                cls_weights=torch.tensor([majority_weight, minority_weight], device=device)
            )
            
            # ============= å…³é”®ä¿®å¤ï¼šä¸ä¼ å…¥rollout_fnå‚æ•° =============
            expl_collector = FedPathCollector(
                policy=policy,
                device=device
                # ç§»é™¤ rollout_fn=rolloutï¼Œä½¿ç”¨é»˜è®¤çš„è‡ªå®šä¹‰rollout
            )
            expl_collector.set_data_loader(data_loader)
            
            eval_collector = FedPathCollector(
                policy=policy,
                device=device
                # ç§»é™¤ rollout_fn=rolloutï¼Œä½¿ç”¨é»˜è®¤çš„è‡ªå®šä¹‰rollout
            )
            eval_collector.set_data_loader(data_loader)
            
            algorithm = TorchBatchRLAlgorithm(
                client_id=client_id,
                trainer=trainer,
                exploration_env=env,
                evaluation_env=env,
                exploration_data_collector=expl_collector,
                evaluation_data_collector=eval_collector,
                replay_buffer=SimpleReplayBuffer(
                    max_size=variant["replay_buffer_size"],
                    max_path_length=variant['algorithm_kwargs']['max_path_length'],
                    env_spec=env.spec  # æ¢å¤env_specå‚æ•°
                ),
                batch_size=variant["batch_size"],
                num_epochs=variant["num_epochs"],
                **variant['algorithm_kwargs']
            )
            algorithms.append(algorithm)
            logging.info(f"å®¢æˆ·ç«¯ {client_id} åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logging.error(f"å®¢æˆ·ç«¯ {client_id} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    # ================== å¼€å§‹è”é‚¦è®­ç»ƒ ==================
    logging.info("å¼€å§‹è”é‚¦è®­ç»ƒ...")
    fed_algorithm = FedAlgorithm(algorithms, variant['num_epochs'], fedFormer=False, patience=5)
    fed_algorithm.train()
    logging.info("è”é‚¦è®­ç»ƒå®Œæˆ")
    
    # ========== æ–°å¢ï¼šAUCåˆ†æéƒ¨åˆ† ==========
    logging.info("\n" + "="*60)
    logging.info("å¼€å§‹AUCè¯¦ç»†åˆ†æ...")
    logging.info("="*60)
    
    try:
        # 1. æ§åˆ¶å°è¾“å‡ºAUCåˆ†æç»“æœ
        auc_analysis = analyze_auc_results(fed_algorithm)
        
        # 2. ç”ŸæˆAUCè¶‹åŠ¿å›¾ï¼ˆå¦‚æœæ”¯æŒå›¾å½¢æ˜¾ç¤ºï¼‰
        try:
            import matplotlib
            matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
            plot_save_path = "results/auc_analysis_plots.png"
            os.makedirs("results", exist_ok=True)
            plot_auc_trends(fed_algorithm, save_path=plot_save_path)
            logging.info(f"AUCè¶‹åŠ¿å›¾å·²ä¿å­˜è‡³: {plot_save_path}")
        except ImportError:
            logging.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾å½¢ç”Ÿæˆ")
        except Exception as e:
            logging.warning(f"ç”ŸæˆAUCå›¾è¡¨æ—¶å‡ºé”™: {str(e)}")
        
        # 3. ç”Ÿæˆè¯¦ç»†çš„AUCåˆ†ææŠ¥å‘Š
        report_save_path = "results/auc_detailed_report.txt"
        os.makedirs("results", exist_ok=True)
        generate_auc_report(fed_algorithm, report_path=report_save_path)
        
        # 4. è¾“å‡ºAUCæ‘˜è¦ä¿¡æ¯åˆ°æ§åˆ¶å°
        global_auc_mean = auc_analysis['global_auc']['mean']
        global_auc_std = auc_analysis['global_auc']['std']
        
        logging.info(f"\nğŸ¯ è”é‚¦å­¦ä¹ AUCæ€§èƒ½æ‘˜è¦:")
        logging.info(f"   å…¨å±€å¹³å‡AUC: {global_auc_mean:.4f} Â± {global_auc_std:.4f}")
        
        # æ‰¾å‡ºæœ€ä½³å®¢æˆ·ç«¯
        best_client = max(auc_analysis['client_auc'].items(), 
                         key=lambda x: x[1]['mean'])
        worst_client = min(auc_analysis['client_auc'].items(), 
                          key=lambda x: x[1]['mean'])
        
        logging.info(f"   æœ€ä½³å®¢æˆ·ç«¯: å®¢æˆ·ç«¯{best_client[0]} (AUC: {best_client[1]['mean']:.4f})")
        logging.info(f"   æœ€å·®å®¢æˆ·ç«¯: å®¢æˆ·ç«¯{worst_client[0]} (AUC: {worst_client[1]['mean']:.4f})")
        logging.info(f"   æ€§èƒ½å·®è·: {best_client[1]['mean'] - worst_client[1]['mean']:.4f}")
        
        # AUCè´¨é‡è¯„ä¼°
        if global_auc_mean >= 0.9:
            auc_quality = "ä¼˜ç§€ ğŸŒŸ"
        elif global_auc_mean >= 0.8:
            auc_quality = "è‰¯å¥½ âœ“"
        elif global_auc_mean >= 0.7:
            auc_quality = "ä¸­ç­‰ â—‹"
        else:
            auc_quality = "éœ€è¦æ”¹è¿› âš ï¸"
        
        logging.info(f"   AUCè´¨é‡è¯„ä¼°: {auc_quality}")
        
    except Exception as e:
        logging.error(f"AUCåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        logging.error("ç»§ç»­ç¨‹åºæ‰§è¡Œï¼Œä½†AUCåˆ†æå¯èƒ½ä¸å®Œæ•´")
    
    logging.info("="*60)
    logging.info("è®­ç»ƒå’Œåˆ†æå…¨éƒ¨å®Œæˆ!")
    logging.info("="*60)

@click.command()
@click.option("--data_root", default="data/clients")  # æŒ‡å‘é¢„å¤„ç†åçš„å®¢æˆ·ç«¯ç›®å½•
@click.option("--num_agents", default=4)
def main(data_root, num_agents):
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler("training.log"),
            logging.StreamHandler()
        ]
    )
    
    variant = dict(
        data_root=data_root,
        num_agents=num_agents,
        batch_size=128,
        policy_hidden_sizes=[256,256],
        layer_norm=True,
        replay_buffer_size=int(1e5),
        num_epochs=50,
        max_steps=1000,
        algorithm_kwargs=dict(
            max_path_length=1000,
            num_eval_steps_per_epoch=1000,
            num_expl_steps_per_train_loop=1000,
            num_trains_per_train_loop=100,
            min_num_steps_before_training=1000,
        )
    )
    
    logging.info("å¼€å§‹å…¥ä¾µæ£€æµ‹è”é‚¦è®­ç»ƒ")
    logging.info(f"é…ç½®å‚æ•°: {variant}")
    
    try:
        experiment(variant)
        logging.info("è®­ç»ƒæˆåŠŸå®Œæˆ")
    except Exception as e:
        logging.error(f"è®­ç»ƒå¤±è´¥: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()